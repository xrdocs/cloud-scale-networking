---
published: false
date: '2018-04-03 20:46 +0200'
title: 'Netflow on NCS5500: Test Results'
author: Nicolas Fevrier
excerpt: Second part of the Netflow on NCS5500 Tuto. This time the test results
tags:
  - ncs5500
  - netflow
  - xr
  - iosxr
Position: hidden
---
{% include toc icon="table" title="Netflow, Sampling-Interval and the Mythical Internet Packet Size" %} 

## Introduction

In this [last blog post on NCS5500 Netflow](https://xrdocs.github.io/cloud-scale-networking/tutorials/2018-02-19-netflow-sampling-interval-and-the-mythical-internet-packet-size/), we presented the details of the implementation from the processes involved to the internal networks used to transport sampled traffic. Also, we provided some details on the various packet size and how should be approached the question on recommended sampling-interval.

Let's test Netflow in lab and see how it behaves when we push all cursors.

## The tests

We will try to check various parameters today, and make sure it doesn't have side effects. For instance, we need to make sure we are not impacting the control planes: the routing protocols handled on the same Line Card CPU should not flap. It's possible to monitor this using various show commands and we will detail them as much as possible for each test. Also, when possible, we will show the impact this test has on LC CPU.

When not mentioned otherwise, these tests are executed on 36x100G-A-SE line cards. The card is fully wired to 100G ports (connected to a testing device able to push line rate traffic over each interface). In some specific tests, we will use a full loaded chassis (16-slots) and here we will use a "snake" configuration where each port is looped to another one to re-inject traffic and load the chassis without requiring 574x 100G testing ports.

Let's get started...

### Impact of the packet size

In this first test, we checking the impact on the CPU load when pushing different packet size.

Test parameters:
- each port is generating 200,000 PPS
- 1M flows total generated by the Ixia
- sample-interval configured: 1:4000

Variable parameter(s):
- packet size: 64B, 128B, 256B, 512B

Measurement:
- CPU impact on nfproducer, nfsrv, netio

Results:

![Picture1.jpg]({{site.baseurl}}/images/Picture1.jpg)

**Comment**: during all this test, we notice that we are collecting different values and that CPU utilization is rarely completely linear. We will need to accept some margin of errors in the figures collected and presented here.
{: .notice--info}

Conclusion: 
- the packet size doesn't seem to influence significantly the CPU load
- it's something we are expecting for everything above 100B @ L3 since the NPU doesn't sample more
- no taildrop observed, no impact on other routing protocols (v4 or v6)

### Impact of the port load / bandwidth

In this second test, we are generating IMIX traffic (with packet size variable between 100B and 300B) and we simply adjust the interface load (simultaneously on the 36x ports).

Test parameters:
- each port is generating 100B-300B packets
- 1M flows total generated by the Ixia
- sample-interval configured: 1:4000

Variable parameter(s):
- interface load: 20%, 40%, 60%, 80%, 100%

Measurement:
- CPU impact on nfproducer, nfsrv, netio

Results:

![Picture2.jpg]({{site.baseurl}}/images/Picture2.jpg)

We can see with some margin of error in the third measurement that CPU load for nfproducer and nfsrv is growing and reach a "plateau". This can be easily explained by the shaper used on each NPU. In current release (6.3.2), we use a shaper of 133Mbps. We will increase this number in next releases (to 200Mbps).

Conclusion:
- bandwidth utilization has, logically, an impact on the CPU load but no taildrop were observed, no impact on other routing protocols (v4 or v6)

### Impact of sampling interval

Here, we will use line rate traffic on the 36 ports and we will only change the sampling-interval in our configuration. It will logically have an impact on the number of sampled packets we push from the NPUs to the Line Card CPU. So we expect the CPU load to grow.

Test parameters:
- each port is generating 512B packets
- each port is transmitting line rate
- we use all 36 ports
- 1M flows total generated by the Ixia

Variable parameter(s):
- sampling interval: 1:32K, 1:16K, 1:8K, 1:4K, 1:2K: 1:1K, 1:1

Measurement:
- CPU impact on nfproducer, nfsrv, netio

Results:

![Picture3.jpg]({{site.baseurl}}/images/Picture3.jpg)

Again, we will need to access some margin of error in the measurement of the last test.

We can see a progression in the CPU utilization, up to a plateau when we reach the shaper. After this value, even if we sample more aggressively, we are not pushing more sampled packets to the LC CPU.

Conclusion:
- with constant traffic, sampling-interval has of course a direct impact on the CPU load but no taildrop were observed, no impact on other routing protocols (v4 or v6)

### Impact of the flow numbers

In this fourth test, we will check what happens when we exceed the cache size. So we will generate more flows than the maximum cache limit we can configure (1 million entries).

Test parameters:
- each port is generating 512B packets
- each port is transmitting line rate
- we use all 36 ports
- sample-interval configured: 1:4000

Variable parameter(s):
- Number of flows: 1M, 2M, 3M

Measurement:
- CPU impact on nfproducer, nfsrv, netio

Results:

![Picture4.jpg]({{site.baseurl}}/images/Picture4.jpg)



